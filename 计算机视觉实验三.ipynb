{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95d3816-8521-42d9-b5c7-ce4988b607e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 将图像转换为灰度图\n",
    "# 读取图像并检查是否成功\n",
    "img_a = cv2.imread(\"c.png\")\n",
    "img_b = cv2.imread(\"d.png\")\n",
    "\n",
    "if img_a is None or img_b is None:\n",
    "    raise ValueError(\"无法读取图像文件。请确保图片路径正确且文件存在。\")\n",
    "\n",
    "# 转换为灰度图\n",
    "gray_a = cv2.cvtColor(img_a, cv2.COLOR_BGR2GRAY)\n",
    "gray_b = cv2.cvtColor(img_b, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "##初始化SIFT检测器并提取特征\n",
    "sift = cv2.SIFT_create()  # 文档中此处为Cv2（大写C），修正为cv2（小写c）\n",
    "kp_a, des_a = sift.detectAndCompute(gray_a, None)  # 图像a的关键点和描述符\n",
    "kp_b, des_b = sift.detectAndCompute(gray_b, None)  # 图像b的关键点和描述符\n",
    "img1_kp = cv2.drawKeypoints(\n",
    "    image=img_a,\n",
    "    keypoints=kp_a,\n",
    "    outImage=None,\n",
    "    color=(0, 255, 0),  # 特征点标记为绿色\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS  # 显示尺度（大小）和方向（箭头）\n",
    ")\n",
    "\n",
    "# 绘制图像2的特征点：彩色标记，显示尺度和方向\n",
    "img2_kp = cv2.drawKeypoints(\n",
    "    image=img_b,\n",
    "    keypoints=kp_b,\n",
    "    outImage=None,\n",
    "    color=(0, 0, 255),  # 特征点标记为红色\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "\n",
    "# 4. 显示或保存可视化结果\n",
    "cv2.imshow(\"Image 1 with SIFT Keypoints\", img1_kp)\n",
    "cv2.imshow(\"Image 2 with SIFT Keypoints\", img2_kp)\n",
    "\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# 使用FLANN匹配器进行特征匹配\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)  # 文档中FLANN后多空格，已修正\n",
    "search_params = dict(checks=50)  # 检查次数越多，匹配越准确但速度越慢\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(des_a, des_b, k=2)  # k=2表示每个特征点返回2个最佳匹配\n",
    "# 应用Lowe's比率测试筛选优质匹配点\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:  # 比率阈值通常取0.7-0.8\n",
    "        good_matches.append(m)\n",
    "# 基本诊断输出\n",
    "print(f\"图像A关键点: {len(kp_a)}, 图像B关键点: {len(kp_b)}\")\n",
    "print(f\"Lowe 筛选后匹配数: {len(good_matches)}\")\n",
    "\n",
    "# 初步绘制匹配（后面会用RANSAC内点重绘）\n",
    "matched_keypoints_img = cv2.drawMatches(\n",
    "    img_a, kp_a, img_b, kp_b, good_matches, None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "# 提取匹配点的坐标\n",
    "src_pts = np.float32([kp_b[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # 图像b的关键点\n",
    "dst_pts = np.float32([kp_a[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # 图像a的关键点\n",
    "\n",
    "# 使用RANSAC算法估计单应矩阵(透视变换矩阵)\n",
    "H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# mask为内点掩码，统计内点数量并在匹配图中只显示内点\n",
    "if mask is None:\n",
    "    print(\"findHomography 未返回掩码，可能匹配不足或失败。\")\n",
    "    inlier_count = 0\n",
    "else:\n",
    "    inlier_count = int(mask.sum())\n",
    "print(f\"RANSAC 内点数量: {inlier_count} / {len(good_matches)}\")\n",
    "\n",
    "# 如果内点过少，提示并提前退出\n",
    "if inlier_count < 4:\n",
    "    raise RuntimeError(\"内点太少，无法计算可靠的单应矩阵（需要至少4个内点）。\")\n",
    "\n",
    "# 使用内点重新绘制匹配图（只显示内点匹配）\n",
    "inlier_matches = [gm for gm, m in zip(good_matches, mask.ravel()) if m]\n",
    "matched_keypoints_img = cv2.drawMatches(\n",
    "    img_a, kp_a, img_b, kp_b, inlier_matches, None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "# 额外诊断信息：打印单应矩阵和数值特性，检测是否存在异常缩放/畸变\n",
    "print(\"单应矩阵 H:\\n\", H)\n",
    "try:\n",
    "    cond_H = np.linalg.cond(H)\n",
    "except Exception:\n",
    "    cond_H = float('inf')\n",
    "print(f\"H 条件数: {cond_H}\")\n",
    "\n",
    "# 获取输入图像尺寸\n",
    "h_a, w_a = img_a.shape[:2]\n",
    "h_b, w_b = img_b.shape[:2]  # 文档中为wb，修正为w_b\n",
    "\n",
    "# 计算图像b变换后的四个角点坐标\n",
    "pts = np.float32([[0, 0], [0, h_b], [w_b, h_b], [w_b, 0]]).reshape(-1, 1, 2)  # 文档中为w_b,e，修正为w_b,0\n",
    "dst_corners = cv2.perspectiveTransform(pts, H)\n",
    "\n",
    "# 确定拼接后图像的最终尺寸(包含所有像素)\n",
    "all_corners = np.concatenate([\n",
    "    dst_corners, \n",
    "    np.float32([[0, 0], [w_a, 0], [w_a, h_a], [0, h_a]]).reshape(-1, 1, 2)  # 文档中缺失部分坐标值，已补充\n",
    "], axis=0)\n",
    "[x_min, y_min] = np.int32(all_corners.min(axis=0).ravel() - 0.5)\n",
    "[x_max, y_max] = np.int32(all_corners.max(axis=0).ravel() + 0.5)\n",
    "\n",
    "# 创建平移矩阵，确保所有像素都在可见区域内\n",
    "translation_matrix = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]], dtype=np.float32)  # 文档中缺失0，已补充\n",
    "\n",
    "# 对图像b进行透视变换和平移\n",
    "fus_img = cv2.warpPerspective(\n",
    "    img_b,\n",
    "    translation_matrix @ H,  # 组合平移矩阵和单应矩阵\n",
    "    (x_max - x_min, y_max - y_min)  # 输出图像尺寸\n",
    ")\n",
    "\n",
    "# 将图像a复制到拼接结果的对应位置\n",
    "fus_img[-y_min:h_a - y_min, -x_min:w_a - x_min] = img_a  # 文档中为xmin，修正为x_min\n",
    "\n",
    "# 显示匹配关键点和拼接结果\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(cv2.cvtColor(img_a, cv2.COLOR_BGR2RGB))\n",
    "plt.title('图像A')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(cv2.cvtColor(img_b, cv2.COLOR_BGR2RGB))\n",
    "plt.title('图像B')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(cv2.cvtColor(matched_keypoints_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('特征匹配')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(cv2.cvtColor(fus_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('拼接结果')\n",
    "plt.show()  # 显示所有图像\n",
    "print(\"按任意键关闭图像窗口...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c85e1-46c8-4919-9828-e0eb9ade3bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
